{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt4sVtx0CqPh"
      },
      "source": [
        "# **Mini-project \\#2 - RegEx Tokenizer**\n",
        "\n",
        "Name: **Ryan Clemence Vasquez**\n",
        "\n",
        "More information on the assessment is found in our Canvas course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d5XeOC1BKkp"
      },
      "source": [
        "# **Loading Data into Memory**\n",
        "\n",
        "This code block is provided and helps load in the data for this assessment from the provided file. Specifically, we are loading in tweets into a Dataframe. The file contains two columns: (1) text and (2) created_at. The 2nd column isn't very useful here and is only really meant to give you an idea of the timeframe these tweets were created. Focus on the 1st columns, but should the time frame help you in your analysis, you're free to analyze that. Also, this code assumes you've placed the tweet csv into the folder where the Python notebook is located.\n",
        "\n",
        "**Note**: Tweets collected here are public tweets. The collection of the tweets was done so through Twitter's API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "3ZCl59JJ8wy8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "tweets = pd.read_csv(\n",
        "  \"tweets_for_mp2.csv\",\n",
        "  index_col=False,\n",
        "  quoting=csv.QUOTE_ALL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "W8t80BvBHVpy"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at_utc+8</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-11-17 23:06:38</td>\n",
              "      <td>HOY YUNG TEASER KINAKABAHAN AKO HAHAHAHAHA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-11-17 23:06:38</td>\n",
              "      <td>Ay unahan po ser? https://t.co/6gdMT3SDwK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-11-17 23:06:39</td>\n",
              "      <td>Huyy wag lang ha hahahaha https://t.co/ob7Ed7MLny</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-11-17 23:06:39</td>\n",
              "      <td>Labyuuuuu ol\\n\\nREQUEST @SB19Official @MTV #Fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-11-17 23:06:39</td>\n",
              "      <td>gusto kaayo nako i share sa page akong letteee...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      created_at_utc+8                                               text\n",
              "0  2020-11-17 23:06:38         HOY YUNG TEASER KINAKABAHAN AKO HAHAHAHAHA\n",
              "1  2020-11-17 23:06:38          Ay unahan po ser? https://t.co/6gdMT3SDwK\n",
              "2  2020-11-17 23:06:39  Huyy wag lang ha hahahaha https://t.co/ob7Ed7MLny\n",
              "3  2020-11-17 23:06:39  Labyuuuuu ol\\n\\nREQUEST @SB19Official @MTV #Fr...\n",
              "4  2020-11-17 23:06:39  gusto kaayo nako i share sa page akong letteee..."
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnzCsnf1E4V1"
      },
      "source": [
        "# **Your Solution**\n",
        "\n",
        "Kindly place your solution in the code block below. Make sure that aside from loading in and tokenizing the data, your code should also display the total tokens, total vocabulary, and list top 25 tokens with their repsective counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "import regex\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Defining the RegEx-based Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "gfXU5Xc4E_ud"
      },
      "outputs": [],
      "source": [
        "class RegExTokenizer:\n",
        "    def __init__(self, pattern=\"\"):\n",
        "        self.pattern = pattern\n",
        "    \n",
        "    def set_pattern(self, pattern):\n",
        "        self.pattern = pattern\n",
        "    \n",
        "    def tokenize(self, text):\n",
        "        if not self.pattern:\n",
        "            raise ValueError(\"No regex pattern provided\")\n",
        "\n",
        "        tokens = regex.findall(self.pattern, text , regex.VERBOSE | regex.I | regex.UNICODE)\n",
        "        return list(tokens)\n",
        "    \n",
        "    def tokenize_df(self, df):\n",
        "        if not self.pattern: \n",
        "            raise ValueError(\"No regex pattern provided\")\n",
        "        \n",
        "        return df.apply(self.tokenize)\n",
        "\n",
        "class TokenCounter: \n",
        "    def __init__(self, source=None):\n",
        "        self.source = source\n",
        "        self.has_processed = False\n",
        "    \n",
        "    def set_tokens(self, new_source):\n",
        "        self.source = new_source\n",
        "        self.has_processed = False\n",
        "\n",
        "    def generate_analysis(self):\n",
        "        if not len(self.source):\n",
        "            raise ValueError(\"No token series provided\")\n",
        "\n",
        "        # Flatten the list of tokens and count the total tokens\n",
        "        self.all_tokens = [token for tokens_list in self.source for token in tokens_list]\n",
        "        self.token_count = len(self.all_tokens)\n",
        "\n",
        "        # Make the vocabulary \n",
        "        self.vocab = set(self.all_tokens)\n",
        "        self.vocab_count = len(self.vocab)\n",
        "\n",
        "        # Count occurences per unique token\n",
        "        self.word_counts = Counter(self.all_tokens)\n",
        "        self.has_processed = True\n",
        "        print(\"Finished analyzing token series...\")\n",
        "\n",
        "    def display_counts(self, k=25): \n",
        "        if not self.has_processed:\n",
        "            raise ValueError(\"Token series not analyzed yet.\")\n",
        "\n",
        "        top_tokens = self.word_counts.most_common(k)\n",
        "        \n",
        "        print(f\"Total tokens: {self.token_count}\")\n",
        "        print(f\"Total vocabulary: {self.vocab_count}\")\n",
        "        print(f\"All words + counts (Top {k} only):\")\n",
        "        \n",
        "        for word, count in top_tokens:\n",
        "            print(f\"{word} : {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Defining the RegEx Pattern to be Used**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "emoticon_string = r\"\"\"\n",
        "(?:\n",
        "  [<>]?\n",
        "  [:;=8]                     # eyes\n",
        "  [\\-o\\*\\']?                 # optional nose\n",
        "  [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
        "  |\n",
        "  [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "  [\\-o\\*\\']?                 # optional nose\n",
        "  [:;=8]                     # eyes\n",
        "  [<>]?\n",
        ")\"\"\"\n",
        "\n",
        "regex_strings = (\n",
        "# URL:\n",
        "r\"\"\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\"\",\n",
        "# Twitter username:\n",
        "r\"\"\"(?:@[\\w_]+)\"\"\",\n",
        "# Hashtags:\n",
        "r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
        ",\n",
        "# Cashtags:\n",
        "r\"\"\"(?:\\$+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
        ",\n",
        "# Remaining word types, respectively:\n",
        "    # Numbers, including fractions, decimals.\n",
        "    # Words, including contractions, and words with numbers too\n",
        "r\"\"\"\n",
        "(?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)|(?:[a-zA-Z0-9\\'-]+)                                 \n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "word_pattern = r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|(?:@[\\w_]+)|(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)|(?:\\$+[\\w_]+[\\w\\'_\\-]*[\\w_]+)|\n",
            "(?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)|(?:[a-zA-Z0-9\\'-]+)                                 \n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(word_pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Simulating the Tokenizer given the Tweets dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = RegExTokenizer(word_pattern)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', \"ain't\", 'lying']"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize(\"I ain't lying.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = tokenizer.tokenize_df(df=tweets['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished analyzing token series...\n"
          ]
        }
      ],
      "source": [
        "ctr = TokenCounter(tokens)\n",
        "ctr.generate_analysis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 2115372\n",
            "Total vocabulary: 295883\n",
            "All words + counts (Top 25 only):\n",
            "na : 42657\n",
            "ko : 32465\n",
            "sa : 30001\n",
            "ng : 19218\n",
            "ako : 17316\n",
            "to : 16982\n",
            "the : 15593\n",
            "I : 14046\n",
            "ang : 13480\n",
            "lang : 13305\n",
            "you : 12156\n",
            "ka : 11543\n",
            "and : 11501\n",
            "a : 11097\n",
            "pa : 10896\n",
            "mo : 10423\n",
            "naman : 9774\n",
            "of : 8727\n",
            "for : 8143\n",
            "mga : 7925\n",
            "is : 7630\n",
            "yung : 7159\n",
            "my : 6855\n",
            "in : 6662\n",
            "at : 6322\n"
          ]
        }
      ],
      "source": [
        "ctr.display_counts()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
