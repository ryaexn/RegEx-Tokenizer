{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt4sVtx0CqPh"
      },
      "source": [
        "# **Mini-project \\#2 - RegEx Tokenizer**\n",
        "\n",
        "Name: **Ryan Clemence Vasquez**\n",
        "\n",
        "More information on the assessment is found in our Canvas course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d5XeOC1BKkp"
      },
      "source": [
        "# **Loading Data into Memory**\n",
        "\n",
        "This code block is provided and helps load in the data for this assessment from the provided file. Specifically, we are loading in tweets into a Dataframe. The file contains two columns: (1) text and (2) created_at. The 2nd column isn't very useful here and is only really meant to give you an idea of the timeframe these tweets were created. Focus on the 1st columns, but should the time frame help you in your analysis, you're free to analyze that. Also, this code assumes you've placed the tweet csv into the folder where the Python notebook is located.\n",
        "\n",
        "**Note**: Tweets collected here are public tweets. The collection of the tweets was done so through Twitter's API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3ZCl59JJ8wy8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "tweets = pd.read_csv(\n",
        "  \"tweets_for_mp2.csv\",\n",
        "  index_col=False,\n",
        "  quoting=csv.QUOTE_ALL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8t80BvBHVpy"
      },
      "outputs": [],
      "source": [
        "tweets.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnzCsnf1E4V1"
      },
      "source": [
        "# **Your Solution**\n",
        "\n",
        "Kindly place your solution in the code block below. Make sure that aside from loading in and tokenizing the data, your code should also display the total tokens, total vocabulary, and list top 25 tokens with their repsective counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import regex\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Defining the RegEx-based Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "metadata": {
        "id": "gfXU5Xc4E_ud"
      },
      "outputs": [],
      "source": [
        "class RegExTokenizer:\n",
        "    def __init__(self, pattern=\"\"):\n",
        "        self.pattern = pattern\n",
        "    \n",
        "    def set_pattern(self, pattern):\n",
        "        self.pattern = pattern\n",
        "    \n",
        "    def tokenize(self, text):\n",
        "        if not self.pattern:\n",
        "            raise ValueError(\"No regex pattern provided\")\n",
        "\n",
        "        tokens = regex.findall(self.pattern, text , regex.VERBOSE | regex.I | regex.UNICODE)\n",
        "        return list(tokens)\n",
        "    \n",
        "    def tokenize_df(self, df):\n",
        "        if not self.pattern: \n",
        "            raise ValueError(\"No regex pattern provided\")\n",
        "        \n",
        "        return df.apply(self.tokenize)\n",
        "\n",
        "class TokenCounter: \n",
        "    def __init__(self, source=None):\n",
        "        self.source = source\n",
        "        self.has_processed = False\n",
        "    \n",
        "    def set_tokens(self, new_source):\n",
        "        self.source = new_source\n",
        "        self.has_processed = False\n",
        "\n",
        "    def generate_analysis(self):\n",
        "        if not len(self.source):\n",
        "            raise ValueError(\"No token series provided\")\n",
        "\n",
        "        # Flatten the list of tokens and count the total tokens\n",
        "        self.all_tokens = [token for tokens_list in self.source for token in tokens_list]\n",
        "        self.token_count = len(self.all_tokens)\n",
        "        \n",
        "        # Normalize capitalized words only\n",
        "        normalized_tokens = [\n",
        "            token.lower() if not token.isupper() else token \n",
        "            for token in self.all_tokens\n",
        "        ]\n",
        "\n",
        "        # Create the vocabulary from the normalized tokens\n",
        "        self.vocab = set(normalized_tokens)\n",
        "        self.vocab_count = len(self.vocab)\n",
        "\n",
        "        # Count occurrences per unique token in the normalized list\n",
        "        self.word_counts = Counter(normalized_tokens)\n",
        "        self.has_processed = True\n",
        "        print(\"Finished analyzing token series...\")\n",
        "\n",
        "        return self.word_counts\n",
        "\n",
        "    def display_counts(self, k=25): \n",
        "        if not self.has_processed:\n",
        "            raise ValueError(\"Token series not analyzed yet.\")\n",
        "\n",
        "        top_tokens = self.word_counts.most_common(k)\n",
        "        \n",
        "        print(f\"Total tokens: {self.token_count}\")\n",
        "        print(f\"Total vocabulary: {self.vocab_count}\")\n",
        "        print(f\"All words + counts (Top {k} only):\")\n",
        "        \n",
        "        for word, count in top_tokens:\n",
        "            print(f\"{word} : {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Defining the RegEx Pattern to be Used**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "metadata": {},
      "outputs": [],
      "source": [
        "emoticon_string = r\"\"\"\n",
        "(?:\n",
        "  [<>]?\n",
        "  [:;=8]                     # eyes\n",
        "  [\\-o\\*\\']?                 # optional nose\n",
        "  [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
        "  |\n",
        "  [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "  [\\-o\\*\\']?                 # optional nose\n",
        "  [:;=8]                     # eyes\n",
        "  [<>]?\n",
        ")\"\"\"\n",
        "\n",
        "# Unicode emoji\n",
        "emoji_string = (\n",
        "              \"[\"\n",
        "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                u\"\\U00002702-\\U000027B0\"\n",
        "                u\"\\U0001f926-\\U0001f937\"\n",
        "                u\"\\U00010000-\\U0010ffff\"\n",
        "                u\"\\u2640-\\u2642\"\n",
        "                u\"\\u2600-\\u2B55\"\n",
        "                u\"\\u200d\"\n",
        "                u\"\\u23cf\"\n",
        "                u\"\\u23e9\"\n",
        "                u\"\\u231a\"\n",
        "                # u\"\\ufe0f\"  # dingbats\n",
        "                # u\"\\u3030\"\n",
        "              \"]\"\n",
        ")\n",
        "\n",
        "# u\"\\U000024C2-\\U0001F251\"\n",
        "\n",
        "regex_strings = (\n",
        "# URL:\n",
        "r\"\"\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\"\",\n",
        "# Twitter username:\n",
        "r\"\"\"(?:@[\\w_]+)\"\"\",\n",
        "# Hashtags:\n",
        "r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
        ",\n",
        "# Cashtags:\n",
        "r\"\"\"(?:\\$+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
        ",\n",
        "# Remaining word types, respectively:\n",
        "    # Numbers, including fractions, decimals.\n",
        "    # Words, including contractions, and words with numbers too\n",
        "r\"\"\"\n",
        "(?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)|(?:[a-zA-Z0-9\\'-]+)                                 \n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "combined_regex_pattern = r\"|\".join([emoji_string] + list(regex_strings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[üòÄ-üôèüåÄ-üóøüöÄ-üõøüá†-üáø‚îÄ-‚ØØ‚úÇ-‚û∞ü§¶-ü§∑êÄÄ-Ùèøø‚ôÄ-‚ôÇ‚òÄ-‚≠ï‚Äç‚èè‚è©‚åö]|http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|(?:@[\\w_]+)|(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)|(?:\\$+[\\w_]+[\\w\\'_\\-]*[\\w_]+)|\n",
            "(?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)|(?:[a-zA-Z0-9\\'-]+)                                 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(combined_regex_pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Simulating the Tokenizer given the Tweets dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = RegExTokenizer(combined_regex_pattern)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'am', 'New', 'York']"
            ]
          },
          "execution_count": 379,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize(text=\"I am New York\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = tokenizer.tokenize_df(df=tweets['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished analyzing token series...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "execution_count": 382,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ctr = TokenCounter(tokens)\n",
        "ctr.generate_analysis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 2307876\n",
            "Total vocabulary: 272067\n",
            "All words + counts (Top 25 only):\n",
            "na : 43112\n",
            "ko : 32507\n",
            "sa : 31016\n",
            "ng : 19363\n",
            "ako : 17934\n",
            "the : 17658\n",
            "to : 17243\n",
            "ang : 16232\n",
            "I : 14046\n",
            "you : 13494\n",
            "lang : 13478\n",
            "üòÇ : 13068\n",
            "and : 12289\n",
            "ka : 11914\n",
            "‚ù§ : 11854\n",
            "üò≠ : 11536\n",
            "pa : 11149\n",
            "a : 11097\n",
            "mo : 10503\n",
            "naman : 9893\n",
            "ü•∫ : 9522\n",
            "of : 8819\n",
            "for : 8556\n",
            "mga : 8281\n",
            "yung : 8198\n"
          ]
        }
      ],
      "source": [
        "ctr.display_counts(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk import TweetTokenizer\n",
        "twt_tokenizer = TweetTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens2 = tweets['text'].apply(twt_tokenizer.tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished analyzing token series...\n"
          ]
        }
      ],
      "source": [
        "ctr.set_tokens(tokens2)\n",
        "ctr.generate_analysis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 2564983\n",
            "Total vocabulary: 274958\n",
            "All words + counts (Top 50 only):\n",
            ". : 75105\n",
            "! : 51925\n",
            "na : 43103\n",
            ", : 41423\n",
            "ko : 32540\n",
            "sa : 31012\n",
            "? : 27509\n",
            "ng : 19360\n",
            "ako : 17920\n",
            "Ô∏è : 17682\n",
            "the : 17662\n",
            "to : 17464\n",
            "ang : 16203\n",
            "I : 14047\n",
            "you : 13495\n",
            "lang : 13476\n",
            "üòÇ : 12517\n",
            "and : 12287\n",
            "ka : 11908\n",
            "‚ù§ : 11640\n",
            "pa : 11141\n",
            "a : 11020\n",
            "mo : 10497\n",
            "üò≠ : 9975\n",
            "naman : 9893\n",
            "ü•∫ : 9071\n",
            "of : 8819\n",
            "for : 8554\n",
            "yung : 8292\n",
            "mga : 8277\n",
            "di : 8255\n",
            "my : 7991\n",
            "is : 7894\n",
            "hahaha : 7283\n",
            "may : 7257\n",
            "ü§£ : 7207\n",
            "@ : 7120\n",
            "in : 7026\n",
            "at : 6773\n",
            "... : 6617\n",
            "‚Äô : 6487\n",
            "mag : 5948\n",
            "it : 5932\n",
            "ni : 5817\n",
            "me : 5784\n",
            "ba : 5593\n",
            "pero : 5546\n",
            "üòç : 5378\n",
            "this : 5329\n",
            "nga : 5295\n"
          ]
        }
      ],
      "source": [
        "ctr.display_counts(50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
