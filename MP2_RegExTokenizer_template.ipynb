{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt4sVtx0CqPh"
      },
      "source": [
        "# **Mini-project \\#2 - RegEx Tokenizer**\n",
        "\n",
        "Name: **Ryan Clemence Vasquez**\n",
        "\n",
        "More information on the assessment is found in our Canvas course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d5XeOC1BKkp"
      },
      "source": [
        "# **Loading Data into Memory**\n",
        "\n",
        "This code block is provided and helps load in the data for this assessment from the provided file. Specifically, we are loading in tweets into a Dataframe. The file contains two columns: (1) text and (2) created_at. The 2nd column isn't very useful here and is only really meant to give you an idea of the timeframe these tweets were created. Focus on the 1st columns, but should the time frame help you in your analysis, you're free to analyze that. Also, this code assumes you've placed the tweet csv into the folder where the Python notebook is located.\n",
        "\n",
        "**Note**: Tweets collected here are public tweets. The collection of the tweets was done so through Twitter's API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "3ZCl59JJ8wy8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "tweets = pd.read_csv(\n",
        "  \"tweets_for_mp2.csv\",\n",
        "  index_col=False,\n",
        "  quoting=csv.QUOTE_ALL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "W8t80BvBHVpy"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at_utc+8</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-11-17 23:06:38</td>\n",
              "      <td>HOY YUNG TEASER KINAKABAHAN AKO HAHAHAHAHA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-11-17 23:06:38</td>\n",
              "      <td>Ay unahan po ser? https://t.co/6gdMT3SDwK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-11-17 23:06:39</td>\n",
              "      <td>Huyy wag lang ha hahahaha https://t.co/ob7Ed7MLny</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-11-17 23:06:39</td>\n",
              "      <td>Labyuuuuu ol\\n\\nREQUEST @SB19Official @MTV #Fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-11-17 23:06:39</td>\n",
              "      <td>gusto kaayo nako i share sa page akong letteee...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      created_at_utc+8                                               text\n",
              "0  2020-11-17 23:06:38         HOY YUNG TEASER KINAKABAHAN AKO HAHAHAHAHA\n",
              "1  2020-11-17 23:06:38          Ay unahan po ser? https://t.co/6gdMT3SDwK\n",
              "2  2020-11-17 23:06:39  Huyy wag lang ha hahahaha https://t.co/ob7Ed7MLny\n",
              "3  2020-11-17 23:06:39  Labyuuuuu ol\\n\\nREQUEST @SB19Official @MTV #Fr...\n",
              "4  2020-11-17 23:06:39  gusto kaayo nako i share sa page akong letteee..."
            ]
          },
          "execution_count": 188,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnzCsnf1E4V1"
      },
      "source": [
        "# **Your Solution**\n",
        "\n",
        "Kindly place your solution in the code block below. Make sure that aside from loading in and tokenizing the data, your code should also display the total tokens, total vocabulary, and list top 25 tokens with their repsective counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "import regex\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Defining the RegEx-based Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "gfXU5Xc4E_ud"
      },
      "outputs": [],
      "source": [
        "class RegExTokenizer:\n",
        "    def __init__(self, pattern=\"\"):\n",
        "        self.pattern = pattern\n",
        "    \n",
        "    def set_pattern(self, pattern):\n",
        "        self.pattern = pattern\n",
        "    \n",
        "    def tokenize(self, text):\n",
        "        if not self.pattern:\n",
        "            raise ValueError(\"No regex pattern provided\")\n",
        "\n",
        "        tokens = regex.findall(self.pattern, text , regex.VERBOSE | regex.I | regex.UNICODE)\n",
        "        return list(tokens)\n",
        "    \n",
        "    def tokenize_df(self, df):\n",
        "        if not self.pattern: \n",
        "            raise ValueError(\"No regex pattern provided\")\n",
        "        \n",
        "        return df.apply(self.tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TokenCounter: \n",
        "    def __init__(self, source=None):\n",
        "        self.source = source\n",
        "        self.has_processed = False\n",
        "    \n",
        "    def set_tokens(self, new_source):\n",
        "        self.source = new_source\n",
        "        self.has_processed = False\n",
        "\n",
        "    def generate_analysis(self):\n",
        "        if not len(self.source):\n",
        "            raise ValueError(\"No token series provided\")\n",
        "\n",
        "        # Flatten the list of tokens and count the total tokens\n",
        "        self.all_tokens = [token for tokens_list in self.source for token in tokens_list]\n",
        "        self.token_count = len(self.all_tokens)\n",
        "    \n",
        "        # Groups capitalized and lower case words together; groups haha-sequences together\n",
        "        def normalize_token(token):\n",
        "\n",
        "            token = token.lower() if not token.isupper() else token\n",
        "            \n",
        "            if regex.fullmatch(r'((haha|ahah)+)[a-z0-9]*', token.lower()):\n",
        "                return \"haha\"\n",
        "            \n",
        "            return token\n",
        "\n",
        "        # Normalize all tokens\n",
        "        normalized_tokens = [normalize_token(token) for token in self.all_tokens]\n",
        "\n",
        "        # Create the vocabulary from the normalized tokens\n",
        "        self.vocab = set(normalized_tokens)\n",
        "        self.vocab_count = len(self.vocab)\n",
        "\n",
        "        # Count occurrences per unique token in the normalized list\n",
        "        self.word_counts = Counter(normalized_tokens)\n",
        "        self.has_processed = True\n",
        "        print(\"Finished analyzing token series...\")\n",
        "\n",
        "        return self.word_counts\n",
        "\n",
        "    def display_counts(self, k=25): \n",
        "        if not self.has_processed:\n",
        "            raise ValueError(\"Token series not analyzed yet.\")\n",
        "\n",
        "        top_tokens = self.word_counts.most_common(k)\n",
        "        \n",
        "        print(f\"Total tokens: {self.token_count}\")\n",
        "        print(f\"Total vocabulary: {self.vocab_count}\")\n",
        "        print( \"------------------------------------\")\n",
        "        print(f\"All words + counts (Top {k} only):\")\n",
        "        \n",
        "        count_df = pd.DataFrame(top_tokens, columns=[\"word\", \"count\"]) \n",
        "        count_df.index = pd.RangeIndex(start=1, stop=len(count_df)+1)\n",
        "\n",
        "        return count_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Defining the RegEx Pattern to be Used**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unicode codes for emoji\n",
        "emoji_string = (\n",
        "              \"[\"\n",
        "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                u\"\\U00002702-\\U000027B0\"\n",
        "                u\"\\U0001f926-\\U0001f937\"\n",
        "                u\"\\U00010000-\\U0010ffff\"\n",
        "                u\"\\u2640-\\u2642\"\n",
        "                u\"\\u2600-\\u2B55\"\n",
        "                u\"\\u200d\"\n",
        "                u\"\\u23cf\"\n",
        "                u\"\\u23e9\"\n",
        "                u\"\\u231a\"\n",
        "              \"]\"\n",
        ")\n",
        "\n",
        "# u\"\\U000024C2-\\U0001F251\"\n",
        "\n",
        "regex_strings = (\n",
        "# URL:\n",
        "r\"\"\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\"\",\n",
        "# Twitter username:\n",
        "r\"\"\"(?:@[\\w_]+)\"\"\",\n",
        "# Hashtags:\n",
        "r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
        ",\n",
        "# Cashtags:\n",
        "r\"\"\"(?:\\$+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
        ",\n",
        "# Remaining word types, respectively:\n",
        "    # Numbers, including fractions, decimals.\n",
        "    # Words, including contractions, and words with numbers too\n",
        "r\"\"\"\n",
        "(?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)|(?:[a-zA-Z0-9\\'-]+)                                 \n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "combined_regex_pattern = r\"|\".join([emoji_string] + list(regex_strings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[😀-🙏🌀-🗿🚀-🛿🇠-🇿─-⯯✂-➰🤦-🤷𐀀-􏿿♀-♂☀-⭕‍⏏⏩⌚]|http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|(?:@[\\w_]+)|(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)|(?:\\$+[\\w_]+[\\w\\'_\\-]*[\\w_]+)|\n",
            "(?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)|(?:[a-zA-Z0-9\\'-]+)                                 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(combined_regex_pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Simulating the Tokenizer with the Tweets Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = RegExTokenizer(combined_regex_pattern)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = tokenizer.tokenize_df(df=tweets['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished analyzing token series...\n"
          ]
        }
      ],
      "source": [
        "ctr = TokenCounter(tokens)\n",
        "tokens2 = ctr.generate_analysis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 2307876\n",
            "Total vocabulary: 269368\n",
            "------------------------------------\n",
            "All words + counts (Top 25 only):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>na</td>\n",
              "      <td>43112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>haha</td>\n",
              "      <td>41303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ko</td>\n",
              "      <td>32507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sa</td>\n",
              "      <td>31016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ng</td>\n",
              "      <td>19363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ako</td>\n",
              "      <td>17934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>the</td>\n",
              "      <td>17658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>to</td>\n",
              "      <td>17243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ang</td>\n",
              "      <td>16232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>I</td>\n",
              "      <td>14046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>you</td>\n",
              "      <td>13494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>lang</td>\n",
              "      <td>13478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>😂</td>\n",
              "      <td>13068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>and</td>\n",
              "      <td>12289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>ka</td>\n",
              "      <td>11914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>❤</td>\n",
              "      <td>11854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>😭</td>\n",
              "      <td>11536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>pa</td>\n",
              "      <td>11149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>a</td>\n",
              "      <td>11097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>mo</td>\n",
              "      <td>10503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>naman</td>\n",
              "      <td>9893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>🥺</td>\n",
              "      <td>9522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>of</td>\n",
              "      <td>8819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>for</td>\n",
              "      <td>8556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>mga</td>\n",
              "      <td>8281</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     word  count\n",
              "1      na  43112\n",
              "2    haha  41303\n",
              "3      ko  32507\n",
              "4      sa  31016\n",
              "5      ng  19363\n",
              "6     ako  17934\n",
              "7     the  17658\n",
              "8      to  17243\n",
              "9     ang  16232\n",
              "10      I  14046\n",
              "11    you  13494\n",
              "12   lang  13478\n",
              "13      😂  13068\n",
              "14    and  12289\n",
              "15     ka  11914\n",
              "16      ❤  11854\n",
              "17      😭  11536\n",
              "18     pa  11149\n",
              "19      a  11097\n",
              "20     mo  10503\n",
              "21  naman   9893\n",
              "22      🥺   9522\n",
              "23     of   8819\n",
              "24    for   8556\n",
              "25    mga   8281"
            ]
          },
          "execution_count": 348,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ctr.display_counts(k=25)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
